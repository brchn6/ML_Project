{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonly_used_start_values True\n",
      "logloss1 - no hyperparameter tunning 0.3421857657330923\n",
      "logloss1 - no hyperparameter tunning with sk_cv -0.20314474061116244\n",
      "num_estimators 311\n",
      "logloss2 - number of estimators native 0.20034467350675253\n",
      "logloss2 - number of estimators with sk_cv -0.20056570947044286\n",
      "{'max_depth': 5, 'min_child_weight': 3}\n",
      "logloss3 - max_depth and min_child_weight with gridsearchcv 0.20055376221165805\n",
      "logloss3 - max_depth and min_child_weight native 0.2003799458390009\n",
      "{'gamma': 1.26}\n",
      "logloss4 - gamma with gridsearchcv 0.19997931229521165\n",
      "logloss4 - gamma native 0.1998566964986054\n",
      "{'colsample_bylevel': 1.0, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "logloss5 - subsample, colsample_bytree and colsample_bylevel a with gridsearchcv 0.19996054410309932\n",
      "logloss5 - subsample, colsample_bytree and colsample_bylevel native 0.20006762277371576\n",
      "{'reg_alpha': 1e-05, 'reg_lambda': 1}\n",
      "logloss6 - reg_alpha and reg_lambda with gridsearchcv 0.19996053937557945\n",
      "logloss6 - reg_alpha and reg_lambda native 0.20006762048477217\n",
      "{'booster': 'gbtree', 'grow_policy': 'depthwise', 'max_delta_step': 0, 'scale_pos_weight': 1.0}\n",
      "logloss7 - scale_pos_weight, max_delta_step, grow_policy, booster with gridsearchcv 0.19996053937557945\n",
      "logloss7 - scale_pos_weight, max_delta_step, grow_policy, booster native 0.20006762048477217\n",
      "{'learning_rate': 0.07}\n",
      "logloss8 - learning_rate with gridsearchcv 0.19992187758099536\n",
      "logloss8 - learning_rate native 0.20627646295201765\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'best_iteration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32860/1447384425.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mxgb_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logloss8 - learning_rate native'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test-logloss-mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfinal_num_estimators\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mxgb_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_num_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/guy_inter/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6292\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6293\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6294\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'best_iteration'"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries and scripts from the pyScripts folder:\n",
    "import time\n",
    "import numpy as np \n",
    "import optuna \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from feature_importance_script import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "#Removing annoying warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Setting main path:\n",
    "path = os.path.join(os.getcwd()) + '/pyScripts/GuyTrain/'\n",
    "\n",
    "#Reading prepared train and test sets (from main.py script):\n",
    "X_train = pd.read_csv(os.path.join(path, 'X_train_df.csv'))\n",
    "X_test = pd.read_csv(os.path.join(path, 'X_test_df.csv'))\n",
    "y_train = pd.read_csv(os.path.join(path, 'y_train.csv'))\n",
    "y_test = pd.read_csv(os.path.join(path, 'y_test.csv'))\n",
    "\n",
    "#Removing columns that were present in the train_set but not in the test_set\n",
    "#This happend due to onehotencoding and extremely rare values that only went to train set:\n",
    "columns = ['diag_3_365.44', 'repaglinide_Down']\n",
    "\n",
    "def removeRogueColumns(df):\n",
    "        df.drop(columns, axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "X_train = removeRogueColumns(X_train)\n",
    "\n",
    "#Calling maim function:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "        start = time.time()\n",
    "    \n",
    "        # choose if want to use set of communly used start values for the hyperparameters \n",
    "        commonly_used_start_values = True \n",
    "        print('commonly_used_start_values', commonly_used_start_values)\n",
    "\n",
    "        if commonly_used_start_values == True: \n",
    "                params = {'device' : \"cuda\", \n",
    "                          'max_depth':5, \n",
    "                          'subsample':0.8, \n",
    "                          'gamma':0, \n",
    "                          'colsample_bytree':0.8,\n",
    "                          'objective' : 'binary:logistic'}\n",
    "                \n",
    "                \n",
    "        else: \n",
    "                params = {}\n",
    "        #Setting the early stopping rounds to find best number of estimators:\n",
    "        early_stopping_rounds=50\n",
    "        #Splitting the train set into train and validation sets:\n",
    "        X_train_es, X_val, y_train_es, y_val = train_test_split(X_train, y_train, shuffle=True, random_state=42)\n",
    "\n",
    "        # DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed. \n",
    "        #Setting the DMatrix for the train set:\n",
    "        dtrain = xgb.DMatrix(X_train, label = y_train, enable_categorical=True) \n",
    "\n",
    "        #Will use stratisfied cross validation in gridsearchcv, this will match our results to the initial results obtained under default parameters.\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Obtaining the untuned logloss score with both sklearn and native xgboost cv, this process will repeat after each hyperparameter tunning step:\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist')\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, nfold=10, metrics='logloss', seed=42, verbose_eval=False, shuffle=True, stratified=True) \n",
    "        print('logloss1 - no hyperparameter tunning native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        xgb_cv_sk = cross_validate(xgb_clf, X_train, y_train, cv=cv, scoring='neg_log_loss')\n",
    "        print('logloss1 - no hyperparameter tunning with sk_cv', xgb_cv_sk['test_score'].mean())\n",
    "        \n",
    "        # Early stopping to find the number of estimator:\n",
    "        xgb_clf = xgb.XGBClassifier(early_stopping_rounds=early_stopping_rounds, **params, random_state=42, enable_categorical=True, n_estimators=1000)\n",
    "        xgb_clf.fit(X_train_es, y_train_es,\n",
    "                eval_set=[(X_val, y_val)], verbose=False, eval_metric='logloss')\n",
    "        \n",
    "        #Saving the best number of estimators:\n",
    "        num_estimators  = xgb_clf.best_iteration\n",
    "        print('num_estimators', num_estimators)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = num_estimators, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True) \n",
    "        print('logloss2 - number of estimators native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        params['n_estimators'] = num_estimators\n",
    "\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist')\n",
    "        xgb_cv_sk = cross_validate(xgb_clf, X_train, y_train, cv=cv, scoring='neg_log_loss')\n",
    "        print('logloss2 - number of estimators with sk_cv', xgb_cv_sk['test_score'].mean())\n",
    "\n",
    "        #Hyperparameter tuning:\n",
    "        #Tuning max_depth and min_child_weight:\n",
    "        param_test1 = {\n",
    "                'max_depth':range(3,20,2),\n",
    "                'min_child_weight':range(1,40,2)\n",
    "        }\n",
    "\n",
    "        gsearch1 = GridSearchCV(param_grid=param_test1, estimator=xgb_clf, scoring='neg_log_loss', cv=cv)  \n",
    "        gsearch1.fit(X_train,y_train)\n",
    "        print(gsearch1.best_params_) \n",
    "        print('logloss3 - max_depth and min_child_weight with gridsearchcv', abs(gsearch1.best_score_))\n",
    "\n",
    "        #Params should be updated before running the native xgboost cv:\n",
    "        #Updating the best parameters:\n",
    "        params.update(gsearch1.best_params_)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = num_estimators, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True)\n",
    "        print('logloss3 - max_depth and min_child_weight native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        #Tuning gamma:\n",
    "        param_test2= {'gamma':np.arange(0, 5, 0.01)}\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist') \n",
    "\n",
    "        gsearch2 = GridSearchCV(param_grid=param_test2, estimator=xgb_clf, scoring='neg_log_loss', cv=cv)\n",
    "        gsearch2.fit(X_train,y_train)\n",
    "        print(gsearch2.best_params_) \n",
    "        print('logloss4 - gamma with gridsearchcv', abs(gsearch2.best_score_))\n",
    "\n",
    "        #Updating the best parameters:\n",
    "        params.update(gsearch2.best_params_)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = num_estimators, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True)\n",
    "        print('logloss4 - gamma native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        #Tuning subsample, colsample_bytree and colsample_bylevel:\n",
    "        param_test3= {'subsample': np.linspace(1, 0.1, 10),\n",
    "                      'colsample_bytree': np.linspace(1, 0.1, 10),\n",
    "                      'colsample_bylevel': np.linspace(1, 0.1, 10)}\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist') \n",
    "\n",
    "        gsearch3 = GridSearchCV(param_grid=param_test3, estimator=xgb_clf, scoring='neg_log_loss', cv=cv)\n",
    "        gsearch3.fit(X_train,y_train)\n",
    "        print(gsearch3.best_params_) \n",
    "        print('logloss5 - subsample, colsample_bytree and colsample_bylevel a with gridsearchcv', abs(gsearch3.best_score_))\n",
    "\n",
    "        #Updating the best parameters:\n",
    "        params.update(gsearch3.best_params_)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = num_estimators, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True)\n",
    "        print('logloss5 - subsample, colsample_bytree and colsample_bylevel native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        #Tuning reg_alpha and reg_lambda:\n",
    "        param_test4 = {'reg_alpha' : [1e-5, 1e-2, 0.1, 1, 1000, 0.001, 0.005, 0.01, 0.05],\n",
    "                       'reg_lambda' : [1e-5, 1e-2, 0.1, 1, 1000, 0.001, 0.005, 0.01, 0.05]}\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist')\n",
    "\n",
    "        gsearch4 = GridSearchCV(param_grid=param_test4, estimator=xgb_clf, scoring='neg_log_loss', cv=cv)\n",
    "        gsearch4.fit(X_train,y_train)\n",
    "        print(gsearch4.best_params_) \n",
    "        print('logloss6 - reg_alpha and reg_lambda with gridsearchcv', abs(gsearch4.best_score_))\n",
    "\n",
    "        #Updating the best parameters:\n",
    "        params.update(gsearch4.best_params_)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = num_estimators, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True)\n",
    "        print('logloss6 - reg_alpha and reg_lambda native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        #Tuning scale_pos_weight, max_delta_step, grow_policy and booster:\n",
    "        param_test5 = {'scale_pos_weight':np.linspace(1,0.1, 10),\n",
    "                       \"max_delta_step\" :  range(0,10,1),\n",
    "                       \"grow_policy\" : [\"depthwise\", \"lossguide\"],\n",
    "                       \"booster\" : [\"gbtree\", \"dart\"]}\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist')               \n",
    "\n",
    "        gsearch5= GridSearchCV(param_grid=param_test5, estimator=xgb_clf, scoring='neg_log_loss', cv=cv)\n",
    "        gsearch5.fit(X_train,y_train)\n",
    "        print(gsearch5.best_params_) \n",
    "        print('logloss7 - scale_pos_weight, max_delta_step, grow_policy, booster with gridsearchcv', abs(gsearch5.best_score_))\n",
    "\n",
    "        #Updating the best parameters:\n",
    "        params.update(gsearch5.best_params_)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = num_estimators, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True)\n",
    "        print('logloss7 - scale_pos_weight, max_delta_step, grow_policy, booster native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        #Tuning learning_rate:\n",
    "        lr = {'learning_rate':np.linspace(0.1,0.01, 10)}\n",
    "        xgb_clf = xgb.XGBClassifier(**params, random_state=42, enable_categorical=True, tree_method='hist')\n",
    "\n",
    "        gsearch6= GridSearchCV(param_grid=lr, estimator=xgb_clf, scoring='neg_log_loss', cv=cv)\n",
    "        gsearch6.fit(X_train,y_train)\n",
    "        print(gsearch6.best_params_) \n",
    "        print('logloss8 - learning_rate with gridsearchcv', abs(gsearch6.best_score_))\n",
    "\n",
    "        #Updating the best parameters:\n",
    "        params.update(gsearch6.best_params_)\n",
    "\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params, num_boost_round = 5000, nfold=10, metrics='logloss',seed=42, verbose_eval=False, shuffle=True, stratified=True)\n",
    "        print('logloss8 - learning_rate native', xgb_cv['test-logloss-mean'].iloc[-1])\n",
    "\n",
    "        #Updating the best number of estimators:\n",
    "        final_num_estimators  = xgb_cv['test-logloss-mean'].idxmin()\n",
    "        params['n_estimators'] = final_num_estimators\n",
    "\n",
    "        best_params = params\n",
    "\n",
    "        #print the best parameters after tunning:\n",
    "        print('best parameters after tunning:', best_params)\n",
    "\n",
    "        #Generarting prediction table and feature importance table on 15 different seeds:\n",
    "        prediction_table = pd.DataFrame()\n",
    "        scores = [log_loss, roc_auc_score]\n",
    "        scores_cm = [precision_score, recall_score, accuracy_score]\n",
    "\n",
    "        for i in range(15):\n",
    "            best_model = xgb.XGBClassifier(**best_params, random_state=i, enable_categorical=True)\n",
    "            best_model.fit(X_train, y_train)\n",
    "            preds_test = best_model.predict_proba(X_test)\n",
    "            preds_cm = best_model.predict(X_test)\n",
    "            for score in scores:\n",
    "                prediction_table.loc['seed_'+str(i), score.__name__] = score(y_test, preds_test[:,1])\n",
    "            for score in scores_cm:\n",
    "                prediction_table.loc['seed_'+str(i), score.__name__] = score(y_test, preds_cm)\n",
    "\n",
    "        #Export the prediction table to a csv file\n",
    "        prediction_table.to_csv('prediction_table.csv')\n",
    "\n",
    "        #Get feature names and feature importance table:\n",
    "        feature_names = best_model.feature_names_in_\n",
    "        fi_table = pd.DataFrame(columns=feature_names)\n",
    "\n",
    "        for i in range(15):\n",
    "            best_model = xgb.XGBClassifier(**best_params, random_state=i, enable_categorical=True, tree_method='hist')\n",
    "            best_model.fit(X_train, y_train)\n",
    "            fi_table.loc['seed_'+str(i)] = best_model.feature_importances_\n",
    "\n",
    "        #Generate fi_plot:\n",
    "        fi_plot = featureImportancePlot(feature_names, fi_table)\n",
    "        \n",
    "        #Export fi_plot to a png file:\n",
    "        fi_plot.savefig('fi_plot.png')\n",
    "\n",
    "        #Dummy classifier on 3 different strategies:\n",
    "        strategies = ['most_frequent', 'uniform', 'constant']\n",
    "        constants = [None, None, 1] \n",
    "\n",
    "        for strategy, constant in zip(strategies, constants):\n",
    "                dummy_clf = DummyClassifier(random_state=42, strategy=strategy, constant=constant)\n",
    "                dummy_clf.fit(X_train, y_train)\n",
    "                dummy_pred = dummy_clf.predict(X_test)\n",
    "                dummy_proba = dummy_clf.predict_proba(X_test)\n",
    "                print('Strategy used: ', strategy)\n",
    "                for score in scores:\n",
    "                        print(score.__name__, round(score(y_test, dummy_proba[:,1]),5))\n",
    "                for score in scores_cm:\n",
    "                        print(score.__name__, round(score(y_test, dummy_pred),5))\n",
    "\n",
    "        #Comparing the best model with default parameters:\n",
    "        def_params = {'device' : \"cuda\", \n",
    "                          'max_depth':5, \n",
    "                          'subsample':0.8, \n",
    "                          'gamma':0, \n",
    "                          'colsample_bytree':0.8,\n",
    "                          'objective' : 'binary:logistic'}\n",
    "\n",
    "        if commonly_used_start_values == True: \n",
    "                xgb_reg = xgb.XGBClassifier(**def_params, random_state=42, enable_categorical=True, tree_method='hist')\n",
    "        else: \n",
    "                xgb_reg = xgb.XGBClassifier(random_state=42, enable_categorical=True, tree_method='hist')\n",
    "        \n",
    "        xgb_reg.fit(X_train, y_train, verbose=False)\n",
    "        y_pred = xgb_reg.predict_proba(X_test)\n",
    "        for score in scores:\n",
    "            score_preds_default = score(y_pred, y_test)\n",
    "            print('logloss-test - no hyperparameter tunning', score_preds_default)\n",
    "            print(\"We have reduced logloss by \", score_preds_default - score(y_test, preds_test[:,1]))   \n",
    "\n",
    "        print(\"Training Time: %s seconds\" % (str(time.time() - start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guy_inter",
   "language": "python",
   "name": "guy_inter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
